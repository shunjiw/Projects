{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training X shape:  (700, 200) \n",
      "Training y shape:  (700,)\n",
      "Testing X shape:  (300, 200) \n",
      "Testing y shape:  (300,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "features = [c for c in train.columns if c not in ['ID_code', 'target']]\n",
    "y = train['target']\n",
    "X = train[features]\n",
    "\n",
    "# Split into training and test data\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.3, random_state = 50)\n",
    "\n",
    "print(\"Training X shape: \", train_X.shape, \"\\nTraining y shape: \", train_y.shape)\n",
    "print(\"Testing X shape: \", test_X.shape, \"\\nTesting y shape: \", test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training and test dataset\n",
    "train_set = lgb.Dataset(data = train_X, label = train_y)\n",
    "test_set = lgb.Dataset(data = test_X, label = test_y)\n",
    "\n",
    "# Define the classifier and grab the default params\n",
    "clf = lgb.LGBMClassifier()\n",
    "default_params = clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weishunji/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:430: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/weishunji/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:741: UserWarning: silent keyword has been found in `params` and will be ignored.\n",
      "Please use silent argument of the Dataset constructor to pass this parameter.\n",
      "  .format(key))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum validation ROC AUC was: 0.63035 with a standard deviation of 0.05757.\n",
      "The optimal number of boosting rounds (estimators) was 92.\n"
     ]
    }
   ],
   "source": [
    "# Baseline ROC AUC with default params\n",
    "\n",
    "# CV to determine the best num_boost_round\n",
    "cv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, \n",
    "                    metrics = 'auc', nfold = 5, seed = 42)\n",
    "\n",
    "print('The maximum validation ROC AUC was: {:.5f} with a standard deviation of {:.5f}.'.format(cv_results['auc-mean'][-1], cv_results['auc-stdv'][-1]))\n",
    "print('The optimal number of boosting rounds (estimators) was {}.'.format(len(cv_results['auc-mean'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline model scores 0.64903 ROC AUC on the test set.\n"
     ]
    }
   ],
   "source": [
    "# Optimal number of esimators\n",
    "clf.n_estimators = len(cv_results['auc-mean'])\n",
    "\n",
    "# Train and make predicions\n",
    "clf.fit(train_X, train_y)\n",
    "preds = clf.predict_proba(test_X)[:, 1]\n",
    "baseline_auc = roc_auc_score(test_y, preds)\n",
    "\n",
    "print('The baseline model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyper-parameters Tuning ###\n",
    "\n",
    "# Define the objective function\n",
    "def objective(hyperparameters):\n",
    "    \n",
    "    # Number of estimators will be found using early stopping\n",
    "    if 'n_estimators' in hyperparameters.keys():\n",
    "        del hyperparameters['n_estimators']\n",
    "    \n",
    "    # Perform n_folds cross validation\n",
    "    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = 5, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = 42)\n",
    "    \n",
    "    # results to return\n",
    "    score = cv_results['auc-mean'][-1]\n",
    "    hyperparameters['n_estimators'] = len(cv_results['auc-mean'])\n",
    "    \n",
    "    return [score, hyperparameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search space\n",
    "param_grid = {\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'class_weight': ['balanced'],\n",
    "    'num_leaves': list(range(20, 150)),\n",
    "    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n",
    "    'subsample_for_bin': list(range(20000, 300000, 20000)),\n",
    "    'min_child_samples': list(range(20, 500, 5)),\n",
    "    'reg_alpha': list(np.linspace(0, 1)),\n",
    "    'reg_lambda': list(np.linspace(0, 1)),\n",
    "    'colsample_bytree': list(np.linspace(0.1, 1, 10)),\n",
    "    'subsample': list(np.linspace(0.1, 1, 100)),\n",
    "    'subsample_freq': list(range(3,10))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum of evaluations\n",
    "MAX_EVALS = 10\n",
    "\n",
    "# Define the random search\n",
    "def random_search(param_grid, max_evals = MAX_EVALS):\n",
    "    \n",
    "    # Dataframe for results\n",
    "    results = pd.DataFrame(columns = ['score', 'params'], index = list(range(MAX_EVALS)))\n",
    "    \n",
    "    # Keep searching until reach max evaluations\n",
    "    for i in range(MAX_EVALS):\n",
    "        \n",
    "        # Choose random hyperparameters\n",
    "        hyperparameters = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "\n",
    "        # Evaluate randomly selected hyperparameters\n",
    "        results.loc[i, :] = objective(hyperparameters)\n",
    "    \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "    \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model from random search scores 0.68110 ROC AUC by cross validation.\n"
     ]
    }
   ],
   "source": [
    "# The parameter combination with best AUC\n",
    "random_search_results = random_search(param_grid)\n",
    "random_search_results.head()\n",
    "\n",
    "print('The best model from random search scores {:.5f} ROC AUC by cross validation.'\n",
    "      .format(random_search_results.loc[0, 'score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model from random search scores 0.58141 ROC AUC on the test set.\n"
     ]
    }
   ],
   "source": [
    "# Get the best parameters\n",
    "best_params = random_search_results.loc[0, 'params']\n",
    "\n",
    "# Fit the model with best parameters\n",
    "clf = lgb.LGBMClassifier(**best_params, random_state = 42)\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Make predictions on test set and print the ROC AUC\n",
    "preds = clf.predict_proba(test_X)[:, 1]\n",
    "\n",
    "print('The best model from random search scores {:.5f} ROC AUC on the test set.'\n",
    "      .format(roc_auc_score(test_y, preds)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

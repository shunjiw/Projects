{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    value = 1/(1+math.exp(-x))\n",
    "    grad = (1-value)*value\n",
    "    return (value, grad)\n",
    "\n",
    "def tanh(x):\n",
    "    value = (math.exp(x) - math.exp(-x))/(math.exp(x) + math.exp(-x))\n",
    "    grad = 1 - value**2\n",
    "    return (value, grad)\n",
    "\n",
    "def ReLU(x):\n",
    "    value = max(0,x)\n",
    "    grad = 0\n",
    "    if x > 0:\n",
    "        grad = 1\n",
    "    return (value, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Shallow Neural Netwrok (1 hidden layer)\n",
    "def shallow_nn_train(X, y, max_steps = 1000, learning_rate = 0.1, n_nodes = 4, activation_func = 'ReLU'):\n",
    "    # Extract the dimension\n",
    "    (n, m) = X.shape\n",
    "\n",
    "    # Set mumber of nodes for each layer\n",
    "    n0 = n\n",
    "    n1 = n_nodes\n",
    "    n2 = 1\n",
    "\n",
    "    # initialize parameters\n",
    "    w1 = np.random.normal(size = (n1,n0))*0.01\n",
    "    b1 = np.zeros((n1,1))\n",
    "    w2 = np.random.normal(size = (n2,n1))*0.01\n",
    "    b2 = np.zeros((n2,1))\n",
    "\n",
    "    z1 = np.dot(w1, X) + b1\n",
    "    a1 = np.fmax(z1, np.zeros(z1.shape)) # ReLU\n",
    "    z2 = np.dot(w2, a1) + b2\n",
    "    a2 = 1/(1+np.exp(-z2)) # Sigmoid\n",
    "    y_hat = a2\n",
    "    \n",
    "    k = 0\n",
    "    \n",
    "    while k < max_steps:\n",
    "        \n",
    "        dz2 = a2 - y\n",
    "        dw2 = np.dot(dz2, a1.T)/m\n",
    "        db2 = np.sum(dz2, axis = 1, keepdims = True)/m\n",
    "        \n",
    "        dz1 = np.multiply(np.dot(w2.T, dz2), (z1 > 0)*1)\n",
    "        dw1 = np.dot(dz1, X.T)/m\n",
    "        db1 = np.sum(dz1, axis = 1, keepdims = True)/m\n",
    "        \n",
    "        w2 -= learning_rate * dw2\n",
    "        b2 -= learning_rate * db2\n",
    "        w1 -= learning_rate * dw1\n",
    "        b1 -= learning_rate * db1\n",
    "        \n",
    "        z1 = np.dot(w1, X) + b1\n",
    "        a1 = np.fmax(z1, np.zeros(z1.shape)) # ReLU\n",
    "        z2 = np.dot(w2, a1) + b2\n",
    "        a2 = 1/(1+np.exp(-z2)) # Sigmoid\n",
    "        \n",
    "        # Earth stop if convergence\n",
    "        if np.mean(abs(y_hat - a2)) < 1e-6:\n",
    "            print('The number of iterations is ', k)\n",
    "            break\n",
    "            \n",
    "        y_hat = a2\n",
    "        k += 1\n",
    "    \n",
    "    if k == max_steps:\n",
    "        print('The number of iterations reach the max_steps', max_steps)\n",
    "        \n",
    "    return (w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_nn_predict(X, w1, b1, w2, b2):\n",
    "    \n",
    "    z1 = np.dot(w1, X) + b1\n",
    "    a1 = np.fmax(z1, np.zeros(z1.shape)) # ReLU\n",
    "    z2 = np.dot(w2, a1) + b2\n",
    "    a2 = 1/(1+np.exp(-z2)) # Sigmoid\n",
    "    \n",
    "    return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate data\n",
    "\n",
    "m_train = 7000 # the number of samples\n",
    "m_test = 3000\n",
    "n = 1000  # the number of features\n",
    "b = 0.5 # the intercept\n",
    "\n",
    "beta = np.array([i/n*2 for i in range(n)]).reshape((n,1))\n",
    "\n",
    "X_train = np.random.normal(size = (n,m_train))\n",
    "X_test = np.random.normal(size = (n,m_test))\n",
    "\n",
    "z_train = np.dot(beta.T, X_train) + b \n",
    "z_test = np.dot(beta.T, X_test) + b \n",
    "\n",
    "y_train = (1/(1+np.exp(-z_train)) > 0.5)*1\n",
    "y_test = (1/(1+np.exp(-z_test)) > 0.5)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 7000), (1, 7000), (1000, 3000), (1, 3000))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of iterations is  4586\n"
     ]
    }
   ],
   "source": [
    "# Train the shallow NN and make predictions\n",
    "w1, b1, w2, b2 = shallow_nn_train(X_train, y_train, max_steps = 10000, n_nodes = 10, learning_rate = 0.05)\n",
    "pred = shallow_nn_predict(X_test, w1, b1, w2, b2)\n",
    "\n",
    "np.mean((pred > 0.5) == y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
